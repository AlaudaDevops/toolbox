<testsuites>
<testsuite name="Worker Node Security Configuration" tests="24" failures="11" errors="0" time="0">
    <testcase name="4.1.1 Ensure that the kubelet service file permissions are set to 600 or more restrictive (Automated)" classname="Worker Node Configuration Files" time="0">
        <failure type="">Run the below command (based on the file location on your system) on the each worker node.&#xA;For example, chmod 600 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;4.1.1&#34;,&#34;test_desc&#34;:&#34;Ensure that the kubelet service file permissions are set to 600 or more restrictive (Automated)&#34;,&#34;audit&#34;:&#34;/bin/sh -c &#39;if test -e /etc/systemd/system/kubelet.service.d/10-kubeadm.conf; then stat -c permissions=%a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf; fi&#39; &#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Run the below command (based on the file location on your system) on the each worker node.\nFor example, chmod 600 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n&#34;,&#34;test_info&#34;:[&#34;Run the below command (based on the file location on your system) on the each worker node.\nFor example, chmod 600 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#39;permissions&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="4.1.2 Ensure that the kubelet service file ownership is set to root:root (Automated)" classname="Worker Node Configuration Files" time="0">
        <system-out>{&#34;test_number&#34;:&#34;4.1.2&#34;,&#34;test_desc&#34;:&#34;Ensure that the kubelet service file ownership is set to root:root (Automated)&#34;,&#34;audit&#34;:&#34;/bin/sh -c \&#34;if test -e /etc/systemd/system/kubelet.service.d/10-kubeadm.conf; then stat -c %U:%G /etc/systemd/system/kubelet.service.d/10-kubeadm.conf; else echo \\\&#34;File not found\\\&#34;; fi\&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Run the below command (based on the file location on your system) on the each worker node.\nFor example,\nchown root:root /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n&#34;,&#34;test_info&#34;:[&#34;Run the below command (based on the file location on your system) on the each worker node.\nFor example,\nchown root:root /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n&#34;],&#34;status&#34;:&#34;PASS&#34;,&#34;actual_value&#34;:&#34;File not found&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#39;root:root&#39; is present OR &#39;File not found&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="4.1.3 If proxy kubeconfig file exists ensure permissions are set to 600 or more restrictive (Manual)" classname="Worker Node Configuration Files" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;4.1.3&#34;,&#34;test_desc&#34;:&#34;If proxy kubeconfig file exists ensure permissions are set to 600 or more restrictive (Manual)&#34;,&#34;audit&#34;:&#34;/bin/sh -c &#39;if test -e /etc/kubernetes/proxy.conf; then stat -c permissions=%a /etc/kubernetes/proxy.conf; fi&#39; &#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Run the below command (based on the file location on your system) on the each worker node.\nFor example,\nchmod 600 /etc/kubernetes/proxy.conf\n&#34;,&#34;test_info&#34;:[&#34;Run the below command (based on the file location on your system) on the each worker node.\nFor example,\nchmod 600 /etc/kubernetes/proxy.conf\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#39;permissions&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="4.1.4 If proxy kubeconfig file exists ensure ownership is set to root:root (Manual)" classname="Worker Node Configuration Files" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;4.1.4&#34;,&#34;test_desc&#34;:&#34;If proxy kubeconfig file exists ensure ownership is set to root:root (Manual)&#34;,&#34;audit&#34;:&#34;/bin/sh -c &#39;if test -e /etc/kubernetes/proxy.conf; then stat -c %U:%G /etc/kubernetes/proxy.conf; fi&#39; &#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Run the below command (based on the file location on your system) on the each worker node.\nFor example, chown root:root /etc/kubernetes/proxy.conf\n&#34;,&#34;test_info&#34;:[&#34;Run the below command (based on the file location on your system) on the each worker node.\nFor example, chown root:root /etc/kubernetes/proxy.conf\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#39;root:root&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="4.1.5 Ensure that the --kubeconfig kubelet.conf file permissions are set to 600 or more restrictive (Automated)" classname="Worker Node Configuration Files" time="0">
        <failure type="">Run the below command (based on the file location on your system) on the each worker node.&#xA;For example,&#xA;chmod 600 /etc/kubernetes/kubelet.conf&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;4.1.5&#34;,&#34;test_desc&#34;:&#34;Ensure that the --kubeconfig kubelet.conf file permissions are set to 600 or more restrictive (Automated)&#34;,&#34;audit&#34;:&#34;/bin/sh -c &#39;if test -e /etc/kubernetes/kubelet.conf; then stat -c permissions=%a /etc/kubernetes/kubelet.conf; fi&#39; &#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Run the below command (based on the file location on your system) on the each worker node.\nFor example,\nchmod 600 /etc/kubernetes/kubelet.conf\n&#34;,&#34;test_info&#34;:[&#34;Run the below command (based on the file location on your system) on the each worker node.\nFor example,\nchmod 600 /etc/kubernetes/kubelet.conf\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#39;permissions&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="4.1.6 Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root (Automated)" classname="Worker Node Configuration Files" time="0">
        <failure type="">Run the below command (based on the file location on your system) on the each worker node.&#xA;For example,&#xA;chown root:root /etc/kubernetes/kubelet.conf&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;4.1.6&#34;,&#34;test_desc&#34;:&#34;Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root (Automated)&#34;,&#34;audit&#34;:&#34;/bin/sh -c &#39;if test -e /etc/kubernetes/kubelet.conf; then stat -c %U:%G /etc/kubernetes/kubelet.conf; fi&#39; &#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Run the below command (based on the file location on your system) on the each worker node.\nFor example,\nchown root:root /etc/kubernetes/kubelet.conf\n&#34;,&#34;test_info&#34;:[&#34;Run the below command (based on the file location on your system) on the each worker node.\nFor example,\nchown root:root /etc/kubernetes/kubelet.conf\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#39;root:root&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="4.1.7 Ensure that the certificate authorities file permissions are set to 600 or more restrictive (Manual)" classname="Worker Node Configuration Files" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;4.1.7&#34;,&#34;test_desc&#34;:&#34;Ensure that the certificate authorities file permissions are set to 600 or more restrictive (Manual)&#34;,&#34;audit&#34;:&#34;CAFILE=$(ps -ef | grep kubelet | grep -v apiserver | grep -- --client-ca-file= | awk -F &#39;--client-ca-file=&#39; &#39;{print $2}&#39; | awk &#39;{print $1}&#39; | uniq)\nif test -z $CAFILE; then CAFILE=/etc/kubernetes/pki/ca.crt; fi\nif test -e $CAFILE; then stat -c permissions=%a $CAFILE; fi\n&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Run the following command to modify the file permissions of the\n--client-ca-file chmod 600 \u003cfilename\u003e\n&#34;,&#34;test_info&#34;:[&#34;Run the following command to modify the file permissions of the\n--client-ca-file chmod 600 \u003cfilename\u003e\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#39;permissions&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="4.1.8 Ensure that the client certificate authorities file ownership is set to root:root (Manual)" classname="Worker Node Configuration Files" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;4.1.8&#34;,&#34;test_desc&#34;:&#34;Ensure that the client certificate authorities file ownership is set to root:root (Manual)&#34;,&#34;audit&#34;:&#34;CAFILE=$(ps -ef | grep kubelet | grep -v apiserver | grep -- --client-ca-file= | awk -F &#39;--client-ca-file=&#39; &#39;{print $2}&#39; | awk &#39;{print $1}&#39; | uniq)\nif test -z $CAFILE; then CAFILE=/etc/kubernetes/pki/ca.crt; fi\nif test -e $CAFILE; then stat -c %U:%G $CAFILE; fi\n&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Run the following command to modify the ownership of the --client-ca-file.\nchown root:root \u003cfilename\u003e\n&#34;,&#34;test_info&#34;:[&#34;Run the following command to modify the ownership of the --client-ca-file.\nchown root:root \u003cfilename\u003e\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#39;root:root&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="4.1.9 If the kubelet config.yaml configuration file is being used validate permissions set to 600 or more restrictive (Automated)" classname="Worker Node Configuration Files" time="0">
        <failure type="">Run the following command (using the config file location identified in the Audit step)&#xA;chmod 600 /var/lib/kubelet/config.yaml&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;4.1.9&#34;,&#34;test_desc&#34;:&#34;If the kubelet config.yaml configuration file is being used validate permissions set to 600 or more restrictive (Automated)&#34;,&#34;audit&#34;:&#34;/bin/sh -c &#39;if test -e /var/lib/kubelet/config.yaml; then stat -c permissions=%a /var/lib/kubelet/config.yaml; fi&#39; &#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Run the following command (using the config file location identified in the Audit step)\nchmod 600 /var/lib/kubelet/config.yaml\n&#34;,&#34;test_info&#34;:[&#34;Run the following command (using the config file location identified in the Audit step)\nchmod 600 /var/lib/kubelet/config.yaml\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#39;permissions&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="4.1.10 If the kubelet config.yaml configuration file is being used validate file ownership is set to root:root (Automated)" classname="Worker Node Configuration Files" time="0">
        <failure type="">Run the following command (using the config file location identified in the Audit step)&#xA;chown root:root /var/lib/kubelet/config.yaml&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;4.1.10&#34;,&#34;test_desc&#34;:&#34;If the kubelet config.yaml configuration file is being used validate file ownership is set to root:root (Automated)&#34;,&#34;audit&#34;:&#34;/bin/sh -c &#39;if test -e /var/lib/kubelet/config.yaml; then stat -c %U:%G /var/lib/kubelet/config.yaml; fi&#39; &#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Run the following command (using the config file location identified in the Audit step)\nchown root:root /var/lib/kubelet/config.yaml\n&#34;,&#34;test_info&#34;:[&#34;Run the following command (using the config file location identified in the Audit step)\nchown root:root /var/lib/kubelet/config.yaml\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#39;root:root&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="4.2.1 Ensure that the --anonymous-auth argument is set to false (Automated)" classname="Kubelet" time="0">
        <failure type="">If using a Kubelet config file, edit the file to set `authentication: anonymous: enabled` to&#xA;`false`.&#xA;If using executable arguments, edit the kubelet service file&#xA;/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and&#xA;set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.&#xA;`--anonymous-auth=false`&#xA;Based on your system, restart the kubelet service. For example,&#xA;systemctl daemon-reload&#xA;systemctl restart kubelet.service&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;4.2.1&#34;,&#34;test_desc&#34;:&#34;Ensure that the --anonymous-auth argument is set to false (Automated)&#34;,&#34;audit&#34;:&#34;/bin/ps -fC $kubeletbin&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;/bin/cat /var/lib/kubelet/config.yaml&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;If using a Kubelet config file, edit the file to set `authentication: anonymous: enabled` to\n`false`.\nIf using executable arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n`--anonymous-auth=false`\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;,&#34;test_info&#34;:[&#34;If using a Kubelet config file, edit the file to set `authentication: anonymous: enabled` to\n`false`.\nIf using executable arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n`--anonymous-auth=false`\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;failed to run: \&#34;/bin/ps -fC $kubeletbin\&#34;, output: \&#34;error: list of command names must follow -C\\n\\nUsage:\\n ps [options]\\n\\n Try &#39;ps --help \u003csimple|list|output|threads|misc|all\u003e&#39;\\n  or &#39;ps --help \u003cs|l|o|t|m|a\u003e&#39;\\n for additional help text.\\n\\nFor more details see ps(1).\\n\&#34;, error: exit status 1&#34;}</system-out>
    </testcase>
    <testcase name="4.2.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated)" classname="Kubelet" time="0">
        <failure type="">If using a Kubelet config file, edit the file to set `authorization.mode` to Webhook. If&#xA;using executable arguments, edit the kubelet service file&#xA;/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and&#xA;set the below parameter in KUBELET_AUTHZ_ARGS variable.&#xA;--authorization-mode=Webhook&#xA;Based on your system, restart the kubelet service. For example,&#xA;systemctl daemon-reload&#xA;systemctl restart kubelet.service&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;4.2.2&#34;,&#34;test_desc&#34;:&#34;Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated)&#34;,&#34;audit&#34;:&#34;/bin/ps -fC $kubeletbin&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;/bin/cat /var/lib/kubelet/config.yaml&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;If using a Kubelet config file, edit the file to set `authorization.mode` to Webhook. If\nusing executable arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_AUTHZ_ARGS variable.\n--authorization-mode=Webhook\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;,&#34;test_info&#34;:[&#34;If using a Kubelet config file, edit the file to set `authorization.mode` to Webhook. If\nusing executable arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_AUTHZ_ARGS variable.\n--authorization-mode=Webhook\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;failed to run: \&#34;/bin/ps -fC $kubeletbin\&#34;, output: \&#34;error: list of command names must follow -C\\n\\nUsage:\\n ps [options]\\n\\n Try &#39;ps --help \u003csimple|list|output|threads|misc|all\u003e&#39;\\n  or &#39;ps --help \u003cs|l|o|t|m|a\u003e&#39;\\n for additional help text.\\n\\nFor more details see ps(1).\\n\&#34;, error: exit status 1&#34;}</system-out>
    </testcase>
    <testcase name="4.2.3 Ensure that the --client-ca-file argument is set as appropriate (Automated)" classname="Kubelet" time="0">
        <failure type="">If using a Kubelet config file, edit the file to set `authentication.x509.clientCAFile` to&#xA;the location of the client CA file.&#xA;If using command line arguments, edit the kubelet service file&#xA;/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and&#xA;set the below parameter in KUBELET_AUTHZ_ARGS variable.&#xA;--client-ca-file=&lt;path/to/client-ca-file&gt;&#xA;Based on your system, restart the kubelet service. For example,&#xA;systemctl daemon-reload&#xA;systemctl restart kubelet.service&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;4.2.3&#34;,&#34;test_desc&#34;:&#34;Ensure that the --client-ca-file argument is set as appropriate (Automated)&#34;,&#34;audit&#34;:&#34;/bin/ps -fC $kubeletbin&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;/bin/cat /var/lib/kubelet/config.yaml&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;If using a Kubelet config file, edit the file to set `authentication.x509.clientCAFile` to\nthe location of the client CA file.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_AUTHZ_ARGS variable.\n--client-ca-file=\u003cpath/to/client-ca-file\u003e\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;,&#34;test_info&#34;:[&#34;If using a Kubelet config file, edit the file to set `authentication.x509.clientCAFile` to\nthe location of the client CA file.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_AUTHZ_ARGS variable.\n--client-ca-file=\u003cpath/to/client-ca-file\u003e\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;failed to run: \&#34;/bin/ps -fC $kubeletbin\&#34;, output: \&#34;error: list of command names must follow -C\\n\\nUsage:\\n ps [options]\\n\\n Try &#39;ps --help \u003csimple|list|output|threads|misc|all\u003e&#39;\\n  or &#39;ps --help \u003cs|l|o|t|m|a\u003e&#39;\\n for additional help text.\\n\\nFor more details see ps(1).\\n\&#34;, error: exit status 1&#34;}</system-out>
    </testcase>
    <testcase name="4.2.4 Verify that the --read-only-port argument is set to 0 (Manual)" classname="Kubelet" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;4.2.4&#34;,&#34;test_desc&#34;:&#34;Verify that the --read-only-port argument is set to 0 (Manual)&#34;,&#34;audit&#34;:&#34;/bin/ps -fC $kubeletbin&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;/bin/cat /var/lib/kubelet/config.yaml&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;If using a Kubelet config file, edit the file to set `readOnlyPort` to 0.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n--read-only-port=0\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;,&#34;test_info&#34;:[&#34;If using a Kubelet config file, edit the file to set `readOnlyPort` to 0.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n--read-only-port=0\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;failed to run: \&#34;/bin/ps -fC $kubeletbin\&#34;, output: \&#34;error: list of command names must follow -C\\n\\nUsage:\\n ps [options]\\n\\n Try &#39;ps --help \u003csimple|list|output|threads|misc|all\u003e&#39;\\n  or &#39;ps --help \u003cs|l|o|t|m|a\u003e&#39;\\n for additional help text.\\n\\nFor more details see ps(1).\\n\&#34;, error: exit status 1&#34;}</system-out>
    </testcase>
    <testcase name="4.2.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Manual)" classname="Kubelet" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;4.2.5&#34;,&#34;test_desc&#34;:&#34;Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Manual)&#34;,&#34;audit&#34;:&#34;/bin/ps -fC $kubeletbin&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;/bin/cat /var/lib/kubelet/config.yaml&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;If using a Kubelet config file, edit the file to set `streamingConnectionIdleTimeout` to a\nvalue other than 0.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n--streaming-connection-idle-timeout=5m\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;,&#34;test_info&#34;:[&#34;If using a Kubelet config file, edit the file to set `streamingConnectionIdleTimeout` to a\nvalue other than 0.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\n--streaming-connection-idle-timeout=5m\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;failed to run: \&#34;/bin/ps -fC $kubeletbin\&#34;, output: \&#34;error: list of command names must follow -C\\n\\nUsage:\\n ps [options]\\n\\n Try &#39;ps --help \u003csimple|list|output|threads|misc|all\u003e&#39;\\n  or &#39;ps --help \u003cs|l|o|t|m|a\u003e&#39;\\n for additional help text.\\n\\nFor more details see ps(1).\\n\&#34;, error: exit status 1&#34;}</system-out>
    </testcase>
    <testcase name="4.2.6 Ensure that the --make-iptables-util-chains argument is set to true (Automated)" classname="Kubelet" time="0">
        <failure type="">If using a Kubelet config file, edit the file to set `makeIPTablesUtilChains` to `true`.&#xA;If using command line arguments, edit the kubelet service file&#xA;/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and&#xA;remove the --make-iptables-util-chains argument from the&#xA;KUBELET_SYSTEM_PODS_ARGS variable.&#xA;Based on your system, restart the kubelet service. For example:&#xA;systemctl daemon-reload&#xA;systemctl restart kubelet.service&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;4.2.6&#34;,&#34;test_desc&#34;:&#34;Ensure that the --make-iptables-util-chains argument is set to true (Automated)&#34;,&#34;audit&#34;:&#34;/bin/ps -fC $kubeletbin&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;/bin/cat /var/lib/kubelet/config.yaml&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;If using a Kubelet config file, edit the file to set `makeIPTablesUtilChains` to `true`.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nremove the --make-iptables-util-chains argument from the\nKUBELET_SYSTEM_PODS_ARGS variable.\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;,&#34;test_info&#34;:[&#34;If using a Kubelet config file, edit the file to set `makeIPTablesUtilChains` to `true`.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nremove the --make-iptables-util-chains argument from the\nKUBELET_SYSTEM_PODS_ARGS variable.\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;failed to run: \&#34;/bin/ps -fC $kubeletbin\&#34;, output: \&#34;error: list of command names must follow -C\\n\\nUsage:\\n ps [options]\\n\\n Try &#39;ps --help \u003csimple|list|output|threads|misc|all\u003e&#39;\\n  or &#39;ps --help \u003cs|l|o|t|m|a\u003e&#39;\\n for additional help text.\\n\\nFor more details see ps(1).\\n\&#34;, error: exit status 1&#34;}</system-out>
    </testcase>
    <testcase name="4.2.7 Ensure that the --hostname-override argument is not set (Manual)" classname="Kubelet" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;4.2.7&#34;,&#34;test_desc&#34;:&#34;Ensure that the --hostname-override argument is not set (Manual)&#34;,&#34;audit&#34;:&#34;/bin/ps -fC $kubeletbin&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\non each worker node and remove the --hostname-override argument from the\nKUBELET_SYSTEM_PODS_ARGS variable.\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;,&#34;test_info&#34;:[&#34;Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\non each worker node and remove the --hostname-override argument from the\nKUBELET_SYSTEM_PODS_ARGS variable.\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;failed to run: \&#34;/bin/ps -fC $kubeletbin\&#34;, output: \&#34;error: list of command names must follow -C\\n\\nUsage:\\n ps [options]\\n\\n Try &#39;ps --help \u003csimple|list|output|threads|misc|all\u003e&#39;\\n  or &#39;ps --help \u003cs|l|o|t|m|a\u003e&#39;\\n for additional help text.\\n\\nFor more details see ps(1).\\n\&#34;, error: exit status 1&#34;}</system-out>
    </testcase>
    <testcase name="4.2.8 Ensure that the eventRecordQPS argument is set to a level which ensures appropriate event capture (Manual)" classname="Kubelet" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;4.2.8&#34;,&#34;test_desc&#34;:&#34;Ensure that the eventRecordQPS argument is set to a level which ensures appropriate event capture (Manual)&#34;,&#34;audit&#34;:&#34;/bin/ps -fC $kubeletbin&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;/bin/cat /var/lib/kubelet/config.yaml&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;If using a Kubelet config file, edit the file to set `eventRecordQPS` to an appropriate level.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;,&#34;test_info&#34;:[&#34;If using a Kubelet config file, edit the file to set `eventRecordQPS` to an appropriate level.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;failed to run: \&#34;/bin/ps -fC $kubeletbin\&#34;, output: \&#34;error: list of command names must follow -C\\n\\nUsage:\\n ps [options]\\n\\n Try &#39;ps --help \u003csimple|list|output|threads|misc|all\u003e&#39;\\n  or &#39;ps --help \u003cs|l|o|t|m|a\u003e&#39;\\n for additional help text.\\n\\nFor more details see ps(1).\\n\&#34;, error: exit status 1&#34;}</system-out>
    </testcase>
    <testcase name="4.2.9 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Manual)" classname="Kubelet" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;4.2.9&#34;,&#34;test_desc&#34;:&#34;Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Manual)&#34;,&#34;audit&#34;:&#34;/bin/ps -fC $kubeletbin&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;/bin/cat /var/lib/kubelet/config.yaml&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;If using a Kubelet config file, edit the file to set `tlsCertFile` to the location\nof the certificate file to use to identify this Kubelet, and `tlsPrivateKeyFile`\nto the location of the corresponding private key file.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameters in KUBELET_CERTIFICATE_ARGS variable.\n--tls-cert-file=\u003cpath/to/tls-certificate-file\u003e\n--tls-private-key-file=\u003cpath/to/tls-key-file\u003e\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;,&#34;test_info&#34;:[&#34;If using a Kubelet config file, edit the file to set `tlsCertFile` to the location\nof the certificate file to use to identify this Kubelet, and `tlsPrivateKeyFile`\nto the location of the corresponding private key file.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the below parameters in KUBELET_CERTIFICATE_ARGS variable.\n--tls-cert-file=\u003cpath/to/tls-certificate-file\u003e\n--tls-private-key-file=\u003cpath/to/tls-key-file\u003e\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;failed to run: \&#34;/bin/ps -fC $kubeletbin\&#34;, output: \&#34;error: list of command names must follow -C\\n\\nUsage:\\n ps [options]\\n\\n Try &#39;ps --help \u003csimple|list|output|threads|misc|all\u003e&#39;\\n  or &#39;ps --help \u003cs|l|o|t|m|a\u003e&#39;\\n for additional help text.\\n\\nFor more details see ps(1).\\n\&#34;, error: exit status 1&#34;}</system-out>
    </testcase>
    <testcase name="4.2.10 Ensure that the --rotate-certificates argument is not set to false (Automated)" classname="Kubelet" time="0">
        <failure type="">If using a Kubelet config file, edit the file to add the line `rotateCertificates` to `true` or&#xA;remove it altogether to use the default value.&#xA;If using command line arguments, edit the kubelet service file&#xA;/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and&#xA;remove --rotate-certificates=false argument from the KUBELET_CERTIFICATE_ARGS&#xA;variable.&#xA;Based on your system, restart the kubelet service. For example,&#xA;systemctl daemon-reload&#xA;systemctl restart kubelet.service&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;4.2.10&#34;,&#34;test_desc&#34;:&#34;Ensure that the --rotate-certificates argument is not set to false (Automated)&#34;,&#34;audit&#34;:&#34;/bin/ps -fC $kubeletbin&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;/bin/cat /var/lib/kubelet/config.yaml&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;If using a Kubelet config file, edit the file to add the line `rotateCertificates` to `true` or\nremove it altogether to use the default value.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nremove --rotate-certificates=false argument from the KUBELET_CERTIFICATE_ARGS\nvariable.\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;,&#34;test_info&#34;:[&#34;If using a Kubelet config file, edit the file to add the line `rotateCertificates` to `true` or\nremove it altogether to use the default value.\nIf using command line arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nremove --rotate-certificates=false argument from the KUBELET_CERTIFICATE_ARGS\nvariable.\nBased on your system, restart the kubelet service. For example,\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;failed to run: \&#34;/bin/ps -fC $kubeletbin\&#34;, output: \&#34;error: list of command names must follow -C\\n\\nUsage:\\n ps [options]\\n\\n Try &#39;ps --help \u003csimple|list|output|threads|misc|all\u003e&#39;\\n  or &#39;ps --help \u003cs|l|o|t|m|a\u003e&#39;\\n for additional help text.\\n\\nFor more details see ps(1).\\n\&#34;, error: exit status 1&#34;}</system-out>
    </testcase>
    <testcase name="4.2.11 Verify that the RotateKubeletServerCertificate argument is set to true (Manual)" classname="Kubelet" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;4.2.11&#34;,&#34;test_desc&#34;:&#34;Verify that the RotateKubeletServerCertificate argument is set to true (Manual)&#34;,&#34;audit&#34;:&#34;/bin/ps -fC $kubeletbin&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;/bin/cat /var/lib/kubelet/config.yaml&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\non each worker node and set the below parameter in KUBELET_CERTIFICATE_ARGS variable.\n--feature-gates=RotateKubeletServerCertificate=true\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;,&#34;test_info&#34;:[&#34;Edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\non each worker node and set the below parameter in KUBELET_CERTIFICATE_ARGS variable.\n--feature-gates=RotateKubeletServerCertificate=true\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;failed to run: \&#34;/bin/ps -fC $kubeletbin\&#34;, output: \&#34;error: list of command names must follow -C\\n\\nUsage:\\n ps [options]\\n\\n Try &#39;ps --help \u003csimple|list|output|threads|misc|all\u003e&#39;\\n  or &#39;ps --help \u003cs|l|o|t|m|a\u003e&#39;\\n for additional help text.\\n\\nFor more details see ps(1).\\n\&#34;, error: exit status 1&#34;}</system-out>
    </testcase>
    <testcase name="4.2.12 Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers (Manual)" classname="Kubelet" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;4.2.12&#34;,&#34;test_desc&#34;:&#34;Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers (Manual)&#34;,&#34;audit&#34;:&#34;/bin/ps -fC $kubeletbin&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;/bin/cat /var/lib/kubelet/config.yaml&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;If using a Kubelet config file, edit the file to set `tlsCipherSuites` to\nTLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\nor to a subset of these values.\nIf using executable arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the --tls-cipher-suites parameter as follows, or to a subset of these values.\n--tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;,&#34;test_info&#34;:[&#34;If using a Kubelet config file, edit the file to set `tlsCipherSuites` to\nTLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\nor to a subset of these values.\nIf using executable arguments, edit the kubelet service file\n/etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and\nset the --tls-cipher-suites parameter as follows, or to a subset of these values.\n--tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;failed to run: \&#34;/bin/ps -fC $kubeletbin\&#34;, output: \&#34;error: list of command names must follow -C\\n\\nUsage:\\n ps [options]\\n\\n Try &#39;ps --help \u003csimple|list|output|threads|misc|all\u003e&#39;\\n  or &#39;ps --help \u003cs|l|o|t|m|a\u003e&#39;\\n for additional help text.\\n\\nFor more details see ps(1).\\n\&#34;, error: exit status 1&#34;}</system-out>
    </testcase>
    <testcase name="4.2.13 Ensure that a limit is set on pod PIDs (Manual)" classname="Kubelet" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;4.2.13&#34;,&#34;test_desc&#34;:&#34;Ensure that a limit is set on pod PIDs (Manual)&#34;,&#34;audit&#34;:&#34;/bin/ps -fC $kubeletbin&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;/bin/cat /var/lib/kubelet/config.yaml&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Decide on an appropriate level for this parameter and set it,\neither via the --pod-max-pids command line parameter or the PodPidsLimit configuration file setting.\n&#34;,&#34;test_info&#34;:[&#34;Decide on an appropriate level for this parameter and set it,\neither via the --pod-max-pids command line parameter or the PodPidsLimit configuration file setting.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;failed to run: \&#34;/bin/ps -fC $kubeletbin\&#34;, output: \&#34;error: list of command names must follow -C\\n\\nUsage:\\n ps [options]\\n\\n Try &#39;ps --help \u003csimple|list|output|threads|misc|all\u003e&#39;\\n  or &#39;ps --help \u003cs|l|o|t|m|a\u003e&#39;\\n for additional help text.\\n\\nFor more details see ps(1).\\n\&#34;, error: exit status 1&#34;}</system-out>
    </testcase>
    <testcase name="4.3.1 Ensure that the kube-proxy metrics service is bound to localhost (Automated)" classname="kube-proxy" time="0">
        <failure type="">Modify or remove any values which bind the metrics service to a non-localhost address.&#xA;The default value is 127.0.0.1:10249.&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;4.3.1&#34;,&#34;test_desc&#34;:&#34;Ensure that the kube-proxy metrics service is bound to localhost (Automated)&#34;,&#34;audit&#34;:&#34;/bin/ps -fC $proxybin&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;/bin/sh -c &#39;if test -e /etc/kubernetes/proxy.conf; then cat /etc/kubernetes/proxy.conf; fi&#39;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Modify or remove any values which bind the metrics service to a non-localhost address.\nThe default value is 127.0.0.1:10249.\n&#34;,&#34;test_info&#34;:[&#34;Modify or remove any values which bind the metrics service to a non-localhost address.\nThe default value is 127.0.0.1:10249.\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;failed to run: \&#34;/bin/ps -fC $proxybin\&#34;, output: \&#34;error: list of command names must follow -C\\n\\nUsage:\\n ps [options]\\n\\n Try &#39;ps --help \u003csimple|list|output|threads|misc|all\u003e&#39;\\n  or &#39;ps --help \u003cs|l|o|t|m|a\u003e&#39;\\n for additional help text.\\n\\nFor more details see ps(1).\\n\&#34;, error: exit status 1&#34;}</system-out>
    </testcase>
</testsuite><testsuite name="Kubernetes Policies" tests="35" failures="6" errors="0" time="0">
    <testcase name="5.1.1 Ensure that the cluster-admin role is only used where required (Automated)" classname="RBAC and Service Accounts" time="0">
        <failure type="">Identify all clusterrolebindings to the cluster-admin role. Check if they are used and&#xA;if they need this role or if they could use a role with fewer privileges.&#xA;Where possible, first bind users to a lower privileged role and then remove the&#xA;clusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name]&#xA;Condition: is_compliant is false if rolename is not cluster-admin and rolebinding is cluster-admin.&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;5.1.1&#34;,&#34;test_desc&#34;:&#34;Ensure that the cluster-admin role is only used where required (Automated)&#34;,&#34;audit&#34;:&#34;kubectl get clusterrolebindings -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name --no-headers | while read -r role_name role_binding subject\ndo\n  if [[ \&#34;${role_name}\&#34; != \&#34;cluster-admin\&#34; \u0026\u0026 \&#34;${role_binding}\&#34; == \&#34;cluster-admin\&#34; ]]; then\n    is_compliant=\&#34;false\&#34;\n  else\n    is_compliant=\&#34;true\&#34;\n  fi;\n  echo \&#34;**role_name: ${role_name} role_binding: ${role_binding} subject: ${subject} is_compliant: ${is_compliant}\&#34;\ndone\n&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Identify all clusterrolebindings to the cluster-admin role. Check if they are used and\nif they need this role or if they could use a role with fewer privileges.\nWhere possible, first bind users to a lower privileged role and then remove the\nclusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name]\nCondition: is_compliant is false if rolename is not cluster-admin and rolebinding is cluster-admin.\n&#34;,&#34;test_info&#34;:[&#34;Identify all clusterrolebindings to the cluster-admin role. Check if they are used and\nif they need this role or if they could use a role with fewer privileges.\nWhere possible, first bind users to a lower privileged role and then remove the\nclusterrolebinding to the cluster-admin role : kubectl delete clusterrolebinding [name]\nCondition: is_compliant is false if rolename is not cluster-admin and rolebinding is cluster-admin.\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;Error from server (Forbidden): clusterrolebindings.rbac.authorization.k8s.io is forbidden: User \&#34;system:serviceaccount:default:default\&#34; cannot list resource \&#34;clusterrolebindings\&#34; in API group \&#34;rbac.authorization.k8s.io\&#34; at the cluster scope&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:true,&#34;expected_result&#34;:&#34;&#39;is_compliant&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="5.1.2 Minimize access to secrets (Automated)" classname="RBAC and Service Accounts" time="0">
        <failure type="">Where possible, remove get, list and watch access to Secret objects in the cluster.&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;5.1.2&#34;,&#34;test_desc&#34;:&#34;Minimize access to secrets (Automated)&#34;,&#34;audit&#34;:&#34;echo \&#34;canGetListWatchSecretsAsSystemAuthenticated: $(kubectl auth can-i get,list,watch secrets --all-namespaces --as=system:authenticated)\&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Where possible, remove get, list and watch access to Secret objects in the cluster.\n&#34;,&#34;test_info&#34;:[&#34;Where possible, remove get, list and watch access to Secret objects in the cluster.\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;E0522 14:36:58.947104   31251 memcache.go:265] \&#34;Unhandled Error\&#34; err=\&#34;couldn&#39;t get current server API group list: Get \\\&#34;http://localhost:8080/api?timeout=32s\\\&#34;: dial tcp [::1]:8080: connect: connection refused\&#34;\nE0522 14:36:58.948353   31251 memcache.go:265] \&#34;Unhandled Error\&#34; err=\&#34;couldn&#39;t get current server API group list: Get \\\&#34;http://localhost:8080/api?timeout=32s\\\&#34;: dial tcp [::1]:8080: connect: connection refused\&#34;\nWarning: the server doesn&#39;t have a resource type &#39;secrets&#39;\n\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\ncanGetListWatchSecretsAsSystemAuthenticated:&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#39;canGetListWatchSecretsAsSystemAuthenticated&#39; is equal to &#39;no&#39;&#34;}</system-out>
    </testcase>
    <testcase name="5.1.3 Minimize wildcard use in Roles and ClusterRoles (Automated)" classname="RBAC and Service Accounts" time="0">
        <failure type="">Where possible replace any use of wildcards [&#34;*&#34;] in roles and clusterroles with specific&#xA;objects or actions.&#xA;Condition: role_is_compliant is false if [&#34;*&#34;] is found in rules.&#xA;Condition: clusterrole_is_compliant is false if [&#34;*&#34;] is found in rules.&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;5.1.3&#34;,&#34;test_desc&#34;:&#34;Minimize wildcard use in Roles and ClusterRoles (Automated)&#34;,&#34;audit&#34;:&#34;# Check Roles\nkubectl get roles --all-namespaces -o custom-columns=ROLE_NAMESPACE:.metadata.namespace,ROLE_NAME:.metadata.name --no-headers | while read -r role_namespace role_name\ndo\n  role_rules=$(kubectl get role -n \&#34;${role_namespace}\&#34; \&#34;${role_name}\&#34; -o=json | jq -c &#39;.rules&#39;)\n  if echo \&#34;${role_rules}\&#34; | grep -q \&#34;\\[\\\&#34;\\*\\\&#34;\\]\&#34;; then\n    role_is_compliant=\&#34;false\&#34;\n  else\n    role_is_compliant=\&#34;true\&#34;\n  fi;\n  echo \&#34;**role_name: ${role_name} role_namespace: ${role_namespace} role_rules: ${role_rules} role_is_compliant: ${role_is_compliant}\&#34;\ndone\n\n# Check ClusterRoles\nkubectl get clusterroles -o custom-columns=CLUSTERROLE_NAME:.metadata.name --no-headers | while read -r clusterrole_name\ndo\n  clusterrole_rules=$(kubectl get clusterrole \&#34;${clusterrole_name}\&#34; -o=json | jq -c &#39;.rules&#39;)\n  if echo \&#34;${clusterrole_rules}\&#34; | grep -q \&#34;\\[\\\&#34;\\*\\\&#34;\\]\&#34;; then\n    clusterrole_is_compliant=\&#34;false\&#34;\n  else\n    clusterrole_is_compliant=\&#34;true\&#34;\n  fi;\necho \&#34;**clusterrole_name: ${clusterrole_name} clusterrole_rules: ${clusterrole_rules} clusterrole_is_compliant: ${clusterrole_is_compliant}\&#34;\ndone\n&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Where possible replace any use of wildcards [\&#34;*\&#34;] in roles and clusterroles with specific\nobjects or actions.\nCondition: role_is_compliant is false if [\&#34;*\&#34;] is found in rules.\nCondition: clusterrole_is_compliant is false if [\&#34;*\&#34;] is found in rules.\n&#34;,&#34;test_info&#34;:[&#34;Where possible replace any use of wildcards [\&#34;*\&#34;] in roles and clusterroles with specific\nobjects or actions.\nCondition: role_is_compliant is false if [\&#34;*\&#34;] is found in rules.\nCondition: clusterrole_is_compliant is false if [\&#34;*\&#34;] is found in rules.\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;Error from server (Forbidden): roles.rbac.authorization.k8s.io is forbidden: User \&#34;system:serviceaccount:default:default\&#34; cannot list resource \&#34;roles\&#34; in API group \&#34;rbac.authorization.k8s.io\&#34; at the cluster scope\nError from server (Forbidden): clusterroles.rbac.authorization.k8s.io is forbidden: User \&#34;system:serviceaccount:default:default\&#34; cannot list resource \&#34;clusterroles\&#34; in API group \&#34;rbac.authorization.k8s.io\&#34; at the cluster scope&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:true,&#34;expected_result&#34;:&#34;&#39;role_is_compliant&#39; is present OR &#39;clusterrole_is_compliant&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="5.1.4 Minimize access to create pods (Automated)" classname="RBAC and Service Accounts" time="0">
        <failure type="">Where possible, remove create access to pod objects in the cluster.&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;5.1.4&#34;,&#34;test_desc&#34;:&#34;Minimize access to create pods (Automated)&#34;,&#34;audit&#34;:&#34;echo \&#34;canCreatePodsAsSystemAuthenticated: $(kubectl auth can-i create pods --all-namespaces --as=system:authenticated)\&#34;\n&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Where possible, remove create access to pod objects in the cluster.\n&#34;,&#34;test_info&#34;:[&#34;Where possible, remove create access to pod objects in the cluster.\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;E0522 14:36:59.009855   31286 memcache.go:265] \&#34;Unhandled Error\&#34; err=\&#34;couldn&#39;t get current server API group list: Get \\\&#34;http://localhost:8080/api?timeout=32s\\\&#34;: dial tcp [::1]:8080: connect: connection refused\&#34;\nE0522 14:36:59.011297   31286 memcache.go:265] \&#34;Unhandled Error\&#34; err=\&#34;couldn&#39;t get current server API group list: Get \\\&#34;http://localhost:8080/api?timeout=32s\\\&#34;: dial tcp [::1]:8080: connect: connection refused\&#34;\nWarning: the server doesn&#39;t have a resource type &#39;pods&#39;\n\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\ncanCreatePodsAsSystemAuthenticated:&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#39;canCreatePodsAsSystemAuthenticated&#39; is equal to &#39;no&#39;&#34;}</system-out>
    </testcase>
    <testcase name="5.1.5 Ensure that default service accounts are not actively used (Automated)" classname="RBAC and Service Accounts" time="0">
        <failure type="">Create explicit service accounts wherever a Kubernetes workload requires specific access&#xA;to the Kubernetes API server.&#xA;Modify the configuration of each default service account to include this value&#xA;`automountServiceAccountToken: false`.&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;5.1.5&#34;,&#34;test_desc&#34;:&#34;Ensure that default service accounts are not actively used (Automated)&#34;,&#34;audit&#34;:&#34;kubectl get serviceaccount --all-namespaces --field-selector metadata.name=default -o=json | jq -r &#39;.items[] | \&#34; namespace: \\(.metadata.namespace), kind: \\(.kind), name: \\(.metadata.name), automountServiceAccountToken: \\(.automountServiceAccountToken | if . == null then \&#34;notset\&#34; else . end )\&#34;&#39; | xargs -L 1\n&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Create explicit service accounts wherever a Kubernetes workload requires specific access\nto the Kubernetes API server.\nModify the configuration of each default service account to include this value\n`automountServiceAccountToken: false`.\n&#34;,&#34;test_info&#34;:[&#34;Create explicit service accounts wherever a Kubernetes workload requires specific access\nto the Kubernetes API server.\nModify the configuration of each default service account to include this value\n`automountServiceAccountToken: false`.\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;Error from server (Forbidden): serviceaccounts \&#34;default\&#34; is forbidden: User \&#34;system:serviceaccount:default:default\&#34; cannot list resource \&#34;serviceaccounts\&#34; in API group \&#34;\&#34; at the cluster scope&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:true,&#34;expected_result&#34;:&#34;&#39;automountServiceAccountToken&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="5.1.6 Ensure that Service Account Tokens are only mounted where necessary (Automated)" classname="RBAC and Service Accounts" time="0">
        <failure type="">Modify the definition of ServiceAccounts and Pods which do not need to mount service&#xA;account tokens to disable it, with `automountServiceAccountToken: false`.&#xA;If both the ServiceAccount and the Pod&#39;s .spec specify a value for automountServiceAccountToken, the Pod spec takes precedence.&#xA;Condition: Pod is_compliant to true when&#xA;  - ServiceAccount is automountServiceAccountToken: false and Pod is automountServiceAccountToken: false or notset&#xA;  - ServiceAccount is automountServiceAccountToken: true notset and Pod is automountServiceAccountToken: false&#xA;</failure>
        <system-out>{&#34;test_number&#34;:&#34;5.1.6&#34;,&#34;test_desc&#34;:&#34;Ensure that Service Account Tokens are only mounted where necessary (Automated)&#34;,&#34;audit&#34;:&#34;kubectl get pods --all-namespaces -o custom-columns=POD_NAMESPACE:.metadata.namespace,POD_NAME:.metadata.name,POD_SERVICE_ACCOUNT:.spec.serviceAccount,POD_IS_AUTOMOUNTSERVICEACCOUNTTOKEN:.spec.automountServiceAccountToken --no-headers | while read -r pod_namespace pod_name pod_service_account pod_is_automountserviceaccounttoken\ndo\n  # Retrieve automountServiceAccountToken&#39;s value for ServiceAccount and Pod, set to notset if null or \u003cnone\u003e.\n  svacc_is_automountserviceaccounttoken=$(kubectl get serviceaccount -n \&#34;${pod_namespace}\&#34; \&#34;${pod_service_account}\&#34; -o json | jq -r &#39;.automountServiceAccountToken&#39; | sed -e &#39;s/\u003cnone\u003e/notset/g&#39; -e &#39;s/null/notset/g&#39;)\n  pod_is_automountserviceaccounttoken=$(echo \&#34;${pod_is_automountserviceaccounttoken}\&#34; | sed -e &#39;s/\u003cnone\u003e/notset/g&#39; -e &#39;s/null/notset/g&#39;)\n  if [ \&#34;${svacc_is_automountserviceaccounttoken}\&#34; = \&#34;false\&#34; ] \u0026\u0026 ( [ \&#34;${pod_is_automountserviceaccounttoken}\&#34; = \&#34;false\&#34; ] || [ \&#34;${pod_is_automountserviceaccounttoken}\&#34; = \&#34;notset\&#34; ] ); then\n    is_compliant=\&#34;true\&#34;\n  elif [ \&#34;${svacc_is_automountserviceaccounttoken}\&#34; = \&#34;true\&#34; ] \u0026\u0026 [ \&#34;${pod_is_automountserviceaccounttoken}\&#34; = \&#34;false\&#34; ]; then\n    is_compliant=\&#34;true\&#34;\n  else\n    is_compliant=\&#34;false\&#34;\n  fi\n  echo \&#34;**namespace: ${pod_namespace} pod_name: ${pod_name} service_account: ${pod_service_account} pod_is_automountserviceaccounttoken: ${pod_is_automountserviceaccounttoken} svacc_is_automountServiceAccountToken: ${svacc_is_automountserviceaccounttoken} is_compliant: ${is_compliant}\&#34;\ndone\n&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Modify the definition of ServiceAccounts and Pods which do not need to mount service\naccount tokens to disable it, with `automountServiceAccountToken: false`.\nIf both the ServiceAccount and the Pod&#39;s .spec specify a value for automountServiceAccountToken, the Pod spec takes precedence.\nCondition: Pod is_compliant to true when\n  - ServiceAccount is automountServiceAccountToken: false and Pod is automountServiceAccountToken: false or notset\n  - ServiceAccount is automountServiceAccountToken: true notset and Pod is automountServiceAccountToken: false\n&#34;,&#34;test_info&#34;:[&#34;Modify the definition of ServiceAccounts and Pods which do not need to mount service\naccount tokens to disable it, with `automountServiceAccountToken: false`.\nIf both the ServiceAccount and the Pod&#39;s .spec specify a value for automountServiceAccountToken, the Pod spec takes precedence.\nCondition: Pod is_compliant to true when\n  - ServiceAccount is automountServiceAccountToken: false and Pod is automountServiceAccountToken: false or notset\n  - ServiceAccount is automountServiceAccountToken: true notset and Pod is automountServiceAccountToken: false\n&#34;],&#34;status&#34;:&#34;FAIL&#34;,&#34;actual_value&#34;:&#34;Error from server (Forbidden): pods is forbidden: User \&#34;system:serviceaccount:default:default\&#34; cannot list resource \&#34;pods\&#34; in API group \&#34;\&#34; at the cluster scope&#34;,&#34;scored&#34;:true,&#34;IsMultiple&#34;:true,&#34;expected_result&#34;:&#34;&#39;is_compliant&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="5.1.7 Avoid use of system:masters group (Manual)" classname="RBAC and Service Accounts" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.1.7&#34;,&#34;test_desc&#34;:&#34;Avoid use of system:masters group (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Remove the system:masters group from all users in the cluster.\n&#34;,&#34;test_info&#34;:[&#34;Remove the system:masters group from all users in the cluster.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.1.8 Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster (Manual)" classname="RBAC and Service Accounts" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.1.8&#34;,&#34;test_desc&#34;:&#34;Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Where possible, remove the impersonate, bind and escalate rights from subjects.\n&#34;,&#34;test_info&#34;:[&#34;Where possible, remove the impersonate, bind and escalate rights from subjects.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.1.9 Minimize access to create persistent volumes (Manual)" classname="RBAC and Service Accounts" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.1.9&#34;,&#34;test_desc&#34;:&#34;Minimize access to create persistent volumes (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Where possible, remove create access to PersistentVolume objects in the cluster.\n&#34;,&#34;test_info&#34;:[&#34;Where possible, remove create access to PersistentVolume objects in the cluster.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.1.10 Minimize access to the proxy sub-resource of nodes (Manual)" classname="RBAC and Service Accounts" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.1.10&#34;,&#34;test_desc&#34;:&#34;Minimize access to the proxy sub-resource of nodes (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Where possible, remove access to the proxy sub-resource of node objects.\n&#34;,&#34;test_info&#34;:[&#34;Where possible, remove access to the proxy sub-resource of node objects.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.1.11 Minimize access to the approval sub-resource of certificatesigningrequests objects (Manual)" classname="RBAC and Service Accounts" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.1.11&#34;,&#34;test_desc&#34;:&#34;Minimize access to the approval sub-resource of certificatesigningrequests objects (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Where possible, remove access to the approval sub-resource of certificatesigningrequests objects.\n&#34;,&#34;test_info&#34;:[&#34;Where possible, remove access to the approval sub-resource of certificatesigningrequests objects.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.1.12 Minimize access to webhook configuration objects (Manual)" classname="RBAC and Service Accounts" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.1.12&#34;,&#34;test_desc&#34;:&#34;Minimize access to webhook configuration objects (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Where possible, remove access to the validatingwebhookconfigurations or mutatingwebhookconfigurations objects\n&#34;,&#34;test_info&#34;:[&#34;Where possible, remove access to the validatingwebhookconfigurations or mutatingwebhookconfigurations objects\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.1.13 Minimize access to the service account token creation (Manual)" classname="RBAC and Service Accounts" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.1.13&#34;,&#34;test_desc&#34;:&#34;Minimize access to the service account token creation (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Where possible, remove access to the token sub-resource of serviceaccount objects.\n&#34;,&#34;test_info&#34;:[&#34;Where possible, remove access to the token sub-resource of serviceaccount objects.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.2.1 Ensure that the cluster has at least one active policy control mechanism in place (Manual)" classname="Pod Security Standards" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.2.1&#34;,&#34;test_desc&#34;:&#34;Ensure that the cluster has at least one active policy control mechanism in place (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Ensure that either Pod Security Admission or an external policy control system is in place\nfor every namespace which contains user workloads.\n&#34;,&#34;test_info&#34;:[&#34;Ensure that either Pod Security Admission or an external policy control system is in place\nfor every namespace which contains user workloads.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.2.2 Minimize the admission of privileged containers (Manual)" classname="Pod Security Standards" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.2.2&#34;,&#34;test_desc&#34;:&#34;Minimize the admission of privileged containers (Manual)&#34;,&#34;audit&#34;:&#34;kubectl get pods --all-namespaces -o custom-columns=POD_NAME:.metadata.name,POD_NAMESPACE:.metadata.namespace --no-headers | while read -r pod_name pod_namespace\ndo\n  # Retrieve container(s) for each Pod.\n  kubectl get pod \&#34;${pod_name}\&#34; --namespace \&#34;${pod_namespace}\&#34; -o json | jq -c &#39;.spec.containers[]&#39; | while read -r container\n  do\n    # Retrieve container&#39;s name.\n    container_name=$(echo ${container} | jq -r &#39;.name&#39;)\n    # Retrieve container&#39;s .securityContext.privileged value.\n    container_privileged=$(echo ${container} | jq -r &#39;.securityContext.privileged&#39; | sed -e &#39;s/null/notset/g&#39;)\n    if [ \&#34;${container_privileged}\&#34; = \&#34;false\&#34; ] || [ \&#34;${container_privileged}\&#34; = \&#34;notset\&#34; ] ; then\n      echo \&#34;***pod_name: ${pod_name} container_name: ${container_name} pod_namespace: ${pod_namespace} is_container_privileged: ${container_privileged} is_compliant: true\&#34;\n    else\n      echo \&#34;***pod_name: ${pod_name} container_name: ${container_name} pod_namespace: ${pod_namespace} is_container_privileged: ${container_privileged} is_compliant: false\&#34;\n    fi\n  done\ndone\n&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of privileged containers.\nAudit: the audit list all pods&#39; containers to retrieve their .securityContext.privileged value.\nCondition: is_compliant is false if container&#39;s `.securityContext.privileged` is set to `true`.\nDefault: by default, there are no restrictions on the creation of privileged containers.\n&#34;,&#34;test_info&#34;:[&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of privileged containers.\nAudit: the audit list all pods&#39; containers to retrieve their .securityContext.privileged value.\nCondition: is_compliant is false if container&#39;s `.securityContext.privileged` is set to `true`.\nDefault: by default, there are no restrictions on the creation of privileged containers.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;Error from server (Forbidden): pods is forbidden: User \&#34;system:serviceaccount:default:default\&#34; cannot list resource \&#34;pods\&#34; in API group \&#34;\&#34; at the cluster scope&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:true,&#34;expected_result&#34;:&#34;&#39;is_compliant&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="5.2.3 Minimize the admission of containers wishing to share the host process ID namespace (Manual)" classname="Pod Security Standards" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.2.3&#34;,&#34;test_desc&#34;:&#34;Minimize the admission of containers wishing to share the host process ID namespace (Manual)&#34;,&#34;audit&#34;:&#34;kubectl get pods --all-namespaces -o custom-columns=POD_NAME:.metadata.name,POD_NAMESPACE:.metadata.namespace --no-headers | while read -r pod_name pod_namespace\ndo\n  # Retrieve spec.hostPID for each pod.\n  pod_hostpid=$(kubectl get pod \&#34;${pod_name}\&#34; --namespace \&#34;${pod_namespace}\&#34; -o jsonpath=&#39;{.spec.hostPID}&#39; 2\u003e/dev/null)\n  if [ -z \&#34;${pod_hostpid}\&#34; ]; then\n    pod_hostpid=\&#34;false\&#34;\n    echo \&#34;***pod_name: ${pod_name} pod_namespace: ${pod_namespace} is_pod_hostpid: ${pod_hostpid} is_compliant: true\&#34;\n  else\n    echo \&#34;***pod_name: ${pod_name} pod_namespace: ${pod_namespace} is_pod_hostpid: ${pod_hostpid} is_compliant: false\&#34;\n  fi\ndone\n&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of `hostPID` containers.\nAudit: the audit retrieves each Pod&#39; spec.hostPID.\nCondition: is_compliant is false if Pod&#39;s spec.hostPID is set to `true`.\nDefault: by default, there are no restrictions on the creation of hostPID containers.\n&#34;,&#34;test_info&#34;:[&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of `hostPID` containers.\nAudit: the audit retrieves each Pod&#39; spec.hostPID.\nCondition: is_compliant is false if Pod&#39;s spec.hostPID is set to `true`.\nDefault: by default, there are no restrictions on the creation of hostPID containers.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;Error from server (Forbidden): pods is forbidden: User \&#34;system:serviceaccount:default:default\&#34; cannot list resource \&#34;pods\&#34; in API group \&#34;\&#34; at the cluster scope&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:true,&#34;expected_result&#34;:&#34;&#39;is_compliant&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="5.2.4 Minimize the admission of containers wishing to share the host IPC namespace (Manual)" classname="Pod Security Standards" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.2.4&#34;,&#34;test_desc&#34;:&#34;Minimize the admission of containers wishing to share the host IPC namespace (Manual)&#34;,&#34;audit&#34;:&#34;kubectl get pods --all-namespaces -o custom-columns=POD_NAME:.metadata.name,POD_NAMESPACE:.metadata.namespace --no-headers | while read -r pod_name pod_namespace\ndo\n  # Retrieve spec.hostIPC for each pod.\n  pod_hostipc=$(kubectl get pod \&#34;${pod_name}\&#34; --namespace \&#34;${pod_namespace}\&#34; -o jsonpath=&#39;{.spec.hostIPC}&#39; 2\u003e/dev/null)\n  if [ -z \&#34;${pod_hostipc}\&#34; ]; then\n    pod_hostipc=\&#34;false\&#34;\n    echo \&#34;***pod_name: ${pod_name} pod_namespace: ${pod_namespace} is_pod_hostipc: ${pod_hostipc} is_compliant: true\&#34;\n  else\n    echo \&#34;***pod_name: ${pod_name} pod_namespace: ${pod_namespace} is_pod_hostipc: ${pod_hostipc} is_compliant: false\&#34;\n  fi\ndone\n&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of `hostIPC` containers.\nAudit: the audit retrieves each Pod&#39; spec.IPC.\nCondition: is_compliant is false if Pod&#39;s spec.hostIPC is set to `true`.\nDefault: by default, there are no restrictions on the creation of hostIPC containers.\n&#34;,&#34;test_info&#34;:[&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of `hostIPC` containers.\nAudit: the audit retrieves each Pod&#39; spec.IPC.\nCondition: is_compliant is false if Pod&#39;s spec.hostIPC is set to `true`.\nDefault: by default, there are no restrictions on the creation of hostIPC containers.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;Error from server (Forbidden): pods is forbidden: User \&#34;system:serviceaccount:default:default\&#34; cannot list resource \&#34;pods\&#34; in API group \&#34;\&#34; at the cluster scope&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:true,&#34;expected_result&#34;:&#34;&#39;is_compliant&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="5.2.5 Minimize the admission of containers wishing to share the host network namespace (Manual)" classname="Pod Security Standards" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.2.5&#34;,&#34;test_desc&#34;:&#34;Minimize the admission of containers wishing to share the host network namespace (Manual)&#34;,&#34;audit&#34;:&#34;kubectl get pods --all-namespaces -o custom-columns=POD_NAME:.metadata.name,POD_NAMESPACE:.metadata.namespace --no-headers | while read -r pod_name pod_namespace\ndo\n  # Retrieve spec.hostNetwork for each pod.\n  pod_hostnetwork=$(kubectl get pod \&#34;${pod_name}\&#34; --namespace \&#34;${pod_namespace}\&#34; -o jsonpath=&#39;{.spec.hostNetwork}&#39; 2\u003e/dev/null)\n  if [ -z \&#34;${pod_hostnetwork}\&#34; ]; then\n    pod_hostnetwork=\&#34;false\&#34;\n    echo \&#34;***pod_name: ${pod_name} pod_namespace: ${pod_namespace} is_pod_hostnetwork: ${pod_hostnetwork} is_compliant: true\&#34;\n  else\n    echo \&#34;***pod_name: ${pod_name} pod_namespace: ${pod_namespace} is_pod_hostnetwork: ${pod_hostnetwork} is_compliant: false\&#34;\n  fi\ndone\n&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of `hostNetwork` containers.\nAudit: the audit retrieves each Pod&#39; spec.hostNetwork.\nCondition: is_compliant is false if Pod&#39;s spec.hostNetwork is set to `true`.\nDefault: by default, there are no restrictions on the creation of hostNetwork containers.\n&#34;,&#34;test_info&#34;:[&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of `hostNetwork` containers.\nAudit: the audit retrieves each Pod&#39; spec.hostNetwork.\nCondition: is_compliant is false if Pod&#39;s spec.hostNetwork is set to `true`.\nDefault: by default, there are no restrictions on the creation of hostNetwork containers.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;Error from server (Forbidden): pods is forbidden: User \&#34;system:serviceaccount:default:default\&#34; cannot list resource \&#34;pods\&#34; in API group \&#34;\&#34; at the cluster scope&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:true,&#34;expected_result&#34;:&#34;&#39;is_compliant&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="5.2.6 Minimize the admission of containers with allowPrivilegeEscalation (Manual)" classname="Pod Security Standards" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.2.6&#34;,&#34;test_desc&#34;:&#34;Minimize the admission of containers with allowPrivilegeEscalation (Manual)&#34;,&#34;audit&#34;:&#34;kubectl get pods --all-namespaces -o custom-columns=POD_NAME:.metadata.name,POD_NAMESPACE:.metadata.namespace --no-headers | while read -r pod_name pod_namespace\ndo\n  # Retrieve container(s) for each Pod.\n  kubectl get pod \&#34;${pod_name}\&#34; --namespace \&#34;${pod_namespace}\&#34; -o json | jq -c &#39;.spec.containers[]&#39; | while read -r container\n  do\n    # Retrieve container&#39;s name\n    container_name=$(echo ${container} | jq -r &#39;.name&#39;)\n    # Retrieve container&#39;s .securityContext.allowPrivilegeEscalation\n    container_allowprivesc=$(echo ${container} | jq -r &#39;.securityContext.allowPrivilegeEscalation&#39; | sed -e &#39;s/null/notset/g&#39;)\n    if [ \&#34;${container_allowprivesc}\&#34; = \&#34;false\&#34; ] || [ \&#34;${container_allowprivesc}\&#34; = \&#34;notset\&#34; ]; then\n      echo \&#34;***pod_name: ${pod_name} container_name: ${container_name} pod_namespace: ${pod_namespace} is_container_allowprivesc: ${container_allowprivesc} is_compliant: true\&#34;\n    else\n      echo \&#34;***pod_name: ${pod_name} container_name: ${container_name} pod_namespace: ${pod_namespace} is_container_allowprivesc: ${container_allowprivesc} is_compliant: false\&#34;\n    fi\n  done\ndone\n&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers with `.securityContext.allowPrivilegeEscalation` set to `true`.\nAudit: the audit retrieves each Pod&#39;s container(s) `.securityContext.allowPrivilegeEscalation`.\nCondition: is_compliant is false if container&#39;s `.securityContext.allowPrivilegeEscalation` is set to `true`.\nDefault: If notset, privilege escalation is allowed (default to true). However if PSP/PSA is used with a `restricted` profile,\nprivilege escalation is explicitly disallowed unless configured otherwise.\n&#34;,&#34;test_info&#34;:[&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers with `.securityContext.allowPrivilegeEscalation` set to `true`.\nAudit: the audit retrieves each Pod&#39;s container(s) `.securityContext.allowPrivilegeEscalation`.\nCondition: is_compliant is false if container&#39;s `.securityContext.allowPrivilegeEscalation` is set to `true`.\nDefault: If notset, privilege escalation is allowed (default to true). However if PSP/PSA is used with a `restricted` profile,\nprivilege escalation is explicitly disallowed unless configured otherwise.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;Error from server (Forbidden): pods is forbidden: User \&#34;system:serviceaccount:default:default\&#34; cannot list resource \&#34;pods\&#34; in API group \&#34;\&#34; at the cluster scope&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:true,&#34;expected_result&#34;:&#34;&#39;is_compliant&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="5.2.7 Minimize the admission of root containers (Manual)" classname="Pod Security Standards" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.2.7&#34;,&#34;test_desc&#34;:&#34;Minimize the admission of root containers (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Create a policy for each namespace in the cluster, ensuring that either `MustRunAsNonRoot`\nor `MustRunAs` with the range of UIDs not including 0, is set.\n&#34;,&#34;test_info&#34;:[&#34;Create a policy for each namespace in the cluster, ensuring that either `MustRunAsNonRoot`\nor `MustRunAs` with the range of UIDs not including 0, is set.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.2.8 Minimize the admission of containers with the NET_RAW capability (Manual)" classname="Pod Security Standards" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.2.8&#34;,&#34;test_desc&#34;:&#34;Minimize the admission of containers with the NET_RAW capability (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers with the `NET_RAW` capability.\n&#34;,&#34;test_info&#34;:[&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers with the `NET_RAW` capability.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.2.9 Minimize the admission of containers with added capabilities (Manual)" classname="Pod Security Standards" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.2.9&#34;,&#34;test_desc&#34;:&#34;Minimize the admission of containers with added capabilities (Manual)&#34;,&#34;audit&#34;:&#34;kubectl get pods --all-namespaces -o custom-columns=POD_NAME:.metadata.name,POD_NAMESPACE:.metadata.namespace --no-headers | while read -r pod_name pod_namespace\ndo\n  # Retrieve container(s) for each Pod.\n  kubectl get pod \&#34;${pod_name}\&#34; --namespace \&#34;${pod_namespace}\&#34; -o json | jq -c &#39;.spec.containers[]&#39; | while read -r container\n  do\n    # Retrieve container&#39;s name\n    container_name=$(echo ${container} | jq -r &#39;.name&#39;)\n    # Retrieve container&#39;s added capabilities\n    container_caps_add=$(echo ${container} | jq -r &#39;.securityContext.capabilities.add&#39; | sed -e &#39;s/null/notset/g&#39;)\n    # Set is_compliant to true by default.\n    is_compliant=true\n    caps_list=\&#34;\&#34;\n    if [ \&#34;${container_caps_add}\&#34; != \&#34;notset\&#34; ]; then\n      # Loop through all caps and append caps_list, then set is_compliant to false.\n      for cap in $(echo \&#34;${container_caps_add}\&#34; | jq -r &#39;.[]&#39;); do\n      caps_list+=\&#34;${cap},\&#34;\n      is_compliant=false\n      done\n      # Remove trailing comma for the last list member.\n      caps_list=${caps_list%,}\n    fi\n    if [ \&#34;${is_compliant}\&#34; = true ]; then\n      echo \&#34;***pod_name: ${pod_name} container_name: ${container_name} pod_namespace: ${pod_namespace} container_caps_add: ${container_caps_add} is_compliant: true\&#34;\n    else\n      echo \&#34;***pod_name: ${pod_name} container_name: ${container_name} pod_namespace: ${pod_namespace} container_caps_add: ${caps_list} is_compliant: false\&#34;\n    fi\n  done\ndone\n&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;&#34;,&#34;remediation&#34;:&#34;Ensure that `allowedCapabilities` is not present in policies for the cluster unless\nit is set to an empty array.\nAudit: the audit retrieves each Pod&#39;s container(s) added capabilities.\nCondition: is_compliant is false if added capabilities are added for a given container.\nDefault: Containers run with a default set of capabilities as assigned by the Container Runtime.\n&#34;,&#34;test_info&#34;:[&#34;Ensure that `allowedCapabilities` is not present in policies for the cluster unless\nit is set to an empty array.\nAudit: the audit retrieves each Pod&#39;s container(s) added capabilities.\nCondition: is_compliant is false if added capabilities are added for a given container.\nDefault: Containers run with a default set of capabilities as assigned by the Container Runtime.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;Error from server (Forbidden): pods is forbidden: User \&#34;system:serviceaccount:default:default\&#34; cannot list resource \&#34;pods\&#34; in API group \&#34;\&#34; at the cluster scope&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:true,&#34;expected_result&#34;:&#34;&#39;is_compliant&#39; is present&#34;}</system-out>
    </testcase>
    <testcase name="5.2.10 Minimize the admission of containers with capabilities assigned (Manual)" classname="Pod Security Standards" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.2.10&#34;,&#34;test_desc&#34;:&#34;Minimize the admission of containers with capabilities assigned (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Review the use of capabilites in applications running on your cluster. Where a namespace\ncontains applications which do not require any Linux capabities to operate consider adding\na PSP which forbids the admission of containers which do not drop all capabilities.\n&#34;,&#34;test_info&#34;:[&#34;Review the use of capabilites in applications running on your cluster. Where a namespace\ncontains applications which do not require any Linux capabities to operate consider adding\na PSP which forbids the admission of containers which do not drop all capabilities.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.2.11 Minimize the admission of Windows HostProcess containers (Manual)" classname="Pod Security Standards" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.2.11&#34;,&#34;test_desc&#34;:&#34;Minimize the admission of Windows HostProcess containers (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers that have `.securityContext.windowsOptions.hostProcess` set to `true`.\n&#34;,&#34;test_info&#34;:[&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers that have `.securityContext.windowsOptions.hostProcess` set to `true`.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.2.12 Minimize the admission of HostPath volumes (Manual)" classname="Pod Security Standards" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.2.12&#34;,&#34;test_desc&#34;:&#34;Minimize the admission of HostPath volumes (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers with `hostPath` volumes.\n&#34;,&#34;test_info&#34;:[&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers with `hostPath` volumes.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.2.13 Minimize the admission of containers which use HostPorts (Manual)" classname="Pod Security Standards" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.2.13&#34;,&#34;test_desc&#34;:&#34;Minimize the admission of containers which use HostPorts (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers which use `hostPort` sections.\n&#34;,&#34;test_info&#34;:[&#34;Add policies to each namespace in the cluster which has user workloads to restrict the\nadmission of containers which use `hostPort` sections.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.3.1 Ensure that the CNI in use supports NetworkPolicies (Manual)" classname="Network Policies and CNI" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.3.1&#34;,&#34;test_desc&#34;:&#34;Ensure that the CNI in use supports NetworkPolicies (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;If the CNI plugin in use does not support network policies, consideration should be given to\nmaking use of a different plugin, or finding an alternate mechanism for restricting traffic\nin the Kubernetes cluster.\n&#34;,&#34;test_info&#34;:[&#34;If the CNI plugin in use does not support network policies, consideration should be given to\nmaking use of a different plugin, or finding an alternate mechanism for restricting traffic\nin the Kubernetes cluster.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.3.2 Ensure that all Namespaces have NetworkPolicies defined (Manual)" classname="Network Policies and CNI" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.3.2&#34;,&#34;test_desc&#34;:&#34;Ensure that all Namespaces have NetworkPolicies defined (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Follow the documentation and create NetworkPolicy objects as you need them.\n&#34;,&#34;test_info&#34;:[&#34;Follow the documentation and create NetworkPolicy objects as you need them.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.4.1 Prefer using Secrets as files over Secrets as environment variables (Manual)" classname="Secrets Management" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.4.1&#34;,&#34;test_desc&#34;:&#34;Prefer using Secrets as files over Secrets as environment variables (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;If possible, rewrite application code to read Secrets from mounted secret files, rather than\nfrom environment variables.\n&#34;,&#34;test_info&#34;:[&#34;If possible, rewrite application code to read Secrets from mounted secret files, rather than\nfrom environment variables.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.4.2 Consider external secret storage (Manual)" classname="Secrets Management" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.4.2&#34;,&#34;test_desc&#34;:&#34;Consider external secret storage (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Refer to the Secrets management options offered by your cloud provider or a third-party\nsecrets management solution.\n&#34;,&#34;test_info&#34;:[&#34;Refer to the Secrets management options offered by your cloud provider or a third-party\nsecrets management solution.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.5.1 Configure Image Provenance using ImagePolicyWebhook admission controller (Manual)" classname="Extensible Admission Control" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.5.1&#34;,&#34;test_desc&#34;:&#34;Configure Image Provenance using ImagePolicyWebhook admission controller (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Follow the Kubernetes documentation and setup image provenance.\n&#34;,&#34;test_info&#34;:[&#34;Follow the Kubernetes documentation and setup image provenance.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.7.1 Create administrative boundaries between resources using namespaces (Manual)" classname="General Policies" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.7.1&#34;,&#34;test_desc&#34;:&#34;Create administrative boundaries between resources using namespaces (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Follow the documentation and create namespaces for objects in your deployment as you need\nthem.\n&#34;,&#34;test_info&#34;:[&#34;Follow the documentation and create namespaces for objects in your deployment as you need\nthem.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.7.2 Ensure that the seccomp profile is set to docker/default in your Pod definitions (Manual)" classname="General Policies" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.7.2&#34;,&#34;test_desc&#34;:&#34;Ensure that the seccomp profile is set to docker/default in your Pod definitions (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Use `securityContext` to enable the docker/default seccomp profile in your pod definitions.\nAn example is as below:\n  securityContext:\n    seccompProfile:\n      type: RuntimeDefault\n&#34;,&#34;test_info&#34;:[&#34;Use `securityContext` to enable the docker/default seccomp profile in your pod definitions.\nAn example is as below:\n  securityContext:\n    seccompProfile:\n      type: RuntimeDefault\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.7.3 Apply SecurityContext to your Pods and Containers (Manual)" classname="General Policies" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.7.3&#34;,&#34;test_desc&#34;:&#34;Apply SecurityContext to your Pods and Containers (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Follow the Kubernetes documentation and apply SecurityContexts to your Pods. For a\nsuggested list of SecurityContexts, you may refer to the CIS Security Benchmark for Docker\nContainers.\n&#34;,&#34;test_info&#34;:[&#34;Follow the Kubernetes documentation and apply SecurityContexts to your Pods. For a\nsuggested list of SecurityContexts, you may refer to the CIS Security Benchmark for Docker\nContainers.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
    <testcase name="5.7.4 The default namespace should not be used (Manual)" classname="General Policies" time="0">
        <skipped></skipped>
        <system-out>{&#34;test_number&#34;:&#34;5.7.4&#34;,&#34;test_desc&#34;:&#34;The default namespace should not be used (Manual)&#34;,&#34;audit&#34;:&#34;&#34;,&#34;AuditEnv&#34;:&#34;&#34;,&#34;AuditConfig&#34;:&#34;&#34;,&#34;type&#34;:&#34;manual&#34;,&#34;remediation&#34;:&#34;Ensure that namespaces are created to allow for appropriate segregation of Kubernetes\nresources and that all new resources are created in a specific namespace.\n&#34;,&#34;test_info&#34;:[&#34;Ensure that namespaces are created to allow for appropriate segregation of Kubernetes\nresources and that all new resources are created in a specific namespace.\n&#34;],&#34;status&#34;:&#34;WARN&#34;,&#34;actual_value&#34;:&#34;&#34;,&#34;scored&#34;:false,&#34;IsMultiple&#34;:false,&#34;expected_result&#34;:&#34;&#34;,&#34;reason&#34;:&#34;Test marked as a manual test&#34;}</system-out>
    </testcase>
</testsuite>
</testsuites>
